{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import snowflake.connector as sf\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket and Snowflake parameters\n",
    "snowflake_account = 'at67872.ap-south-1.aws'\n",
    "snowflake_user = 'FYP4'\n",
    "snowflake_password = '!Q2w3e4r5t6y'\n",
    "warehouse='COMPUTE_WH'\n",
    "snowflake_database = 'COAPDATA'\n",
    "snowflake_schema = 'METER_SCHEMA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Snowflake\n",
    "conn = sf.connect(\n",
    "        user=snowflake_user,\n",
    "        password=snowflake_password,\n",
    "        account=snowflake_account,\n",
    "        warehouse=warehouse,\n",
    "        database=snowflake_database,\n",
    "        schema=snowflake_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of water meter views\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SHOW VIEWS IN SCHEMA {snowflake_database}.{snowflake_schema}\")\n",
    "views = [row[1] for row in cursor.fetchall()]\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'METER_1_VIEW'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the dataframes for each water meter view\n",
    "view_dataframes = {}\n",
    "dataframes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each water meter view\n",
    "for view in views:\n",
    "    # Retrieve the data from the current water meter view\n",
    "    query = f\"SELECT Date, total FROM {snowflake_database}.{snowflake_schema}.{view} ORDER BY Date DESC\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "    columns = [column[0] for column in cursor.description]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    dataframes.append(df)\n",
    "        \n",
    "    # # # Scale the data using standardization\n",
    "    # scaler = StandardScaler()\n",
    "    # # Convert the 'total' column to a numpy array\n",
    "    # data = df['TOTAL'].values \n",
    "    # scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "        \n",
    "    # # # Store the dataframe in the dictionary\n",
    "    # view_dataframes[view] = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale each dataframe using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_dataframes = []\n",
    "\n",
    "for df in dataframes:\n",
    "    df = df['TOTAL'].values \n",
    "    scaled_data = scaler.fit_transform(df.reshape(-1, 1))\n",
    "    # scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "    scaled_dataframes.append(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-07</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  TOTAL\n",
       "0  2023-06-07    535\n",
       "1  2023-06-01    250\n",
       "2  2023-01-04    255\n",
       "3  2022-01-04    245"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.73134044],\n",
       "       [-0.57711348],\n",
       "       [-0.53661429],\n",
       "       [-0.61761267]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access each scaled dataframe\n",
    "df1_scaled = scaled_dataframes[0]\n",
    "df1_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14360/786057323.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaled_dataframes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf2_scaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "df2_scaled = scaled_dataframes[1]\n",
    "df2_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_layer = Input(shape=(1,))\n",
    "\n",
    "# Define the encoder layers\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder layers\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(1, activation='linear')(decoded)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 1.1708\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1431\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1160\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0891\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0627\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0367\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0110\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 994us/step - loss: 0.9857\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9609\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9365\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9125\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8888\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8656\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8428\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.8204\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7984\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7768\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.7555\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7347\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.7147\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6950\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6757\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6567\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6381\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6199\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6023\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5852\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5685\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5521\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5363\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5213\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5065\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4920\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4779\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4640\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4505\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4373\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4244\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4118\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3994\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3873\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3754\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3638\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3524\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3413\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3305\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3198\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3094\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2992\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2893\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2796\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2701\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2608\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2517\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2429\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2345\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2263\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2183\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2104\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 867us/step - loss: 0.2028\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1953\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1880\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1809\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1740\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1672\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1606\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1542\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1479\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1419\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1359\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1302\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1246\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1191\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1138\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1087\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1037\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0989\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0942\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0897\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0854\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0811\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0771\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.0731\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0693\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0657\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0621\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0588\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0555\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0524\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0494\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0465\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0438\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0412\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0387\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0363\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0340\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0318\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0297\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0259\n"
     ]
    }
   ],
   "source": [
    "# List to store the trained autoencoders\n",
    "autoencoders = []\n",
    "\n",
    "# Define the autoencoder model\n",
    "input_dim = scaled_dataframes[0].shape[1]\n",
    "encoding_dim = 64\n",
    "\n",
    "for scaled_df in scaled_dataframes:\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoder = layers.Dense(input_dim, activation='linear')(encoder)\n",
    "\n",
    "    autoencoder = tf.keras.Model(input_layer, decoder)\n",
    "\n",
    "    # Compile and train the autoencoder\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(scaled_df, scaled_df, epochs=100, batch_size=32)\n",
    "\n",
    "    # Add trained autoencoder to the list\n",
    "    autoencoders.append(autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08174077590330205\n"
     ]
    }
   ],
   "source": [
    "anomaly_threshold = []  # Adjust this threshold as needed\n",
    "threshold=[]\n",
    "\n",
    "anomalies = []\n",
    "\n",
    "# Iterate over each scaled dataframe and its corresponding autoencoder\n",
    "for scaled_df, autoencoder in zip(scaled_dataframes, autoencoders):\n",
    "    # Reconstruct the data using the autoencoder\n",
    "    reconstructed_data = autoencoder.predict(scaled_df)\n",
    "\n",
    "    # Calculate the reconstruction error\n",
    "    reconstruction_error = np.mean(np.square(scaled_df - reconstructed_data), axis=1)\n",
    "\n",
    "    # Calculate the threshold dynamically\n",
    "    threshold = np.percentile(reconstruction_error, 95)\n",
    "    print(threshold)\n",
    "\n",
    "    # Identify anomalies based on the threshold\n",
    "    view_anomalies = scaled_df[reconstruction_error > threshold]\n",
    "\n",
    "    # Add the anomalies to the list\n",
    "    anomalies.append(view_anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73134044]\n"
     ]
    }
   ],
   "source": [
    "# List to store anomaly flags\n",
    "anomaly_flags = []\n",
    "\n",
    "# Iterate over each view and its corresponding autoencoder and scaled dataframe\n",
    "for autoencoder, scaled_df in zip(autoencoders, scaled_dataframes):\n",
    "    # Get the last entry of the scaled dataframe\n",
    "    last_entry = scaled_df[0]\n",
    "    print (last_entry)\n",
    "\n",
    "    # Reshape the last entry to match the autoencoder input shape\n",
    "    last_entry = last_entry.reshape(1, -1)\n",
    "\n",
    "    # Reconstruct the data using the autoencoder\n",
    "    reconstructed_data = autoencoder.predict(last_entry)\n",
    "\n",
    "    # Calculate the reconstruction error\n",
    "    reconstruction_error = np.mean(np.square(last_entry - reconstructed_data))\n",
    "\n",
    "    # Check if the reconstruction error exceeds the threshold for anomaly detection\n",
    "    #anomaly_threshold = 0.01  # Adjust the threshold as needed\n",
    "    is_anomaly = int(reconstruction_error > threshold)\n",
    "\n",
    "    # Store the anomaly flag\n",
    "    anomaly_flags.append(is_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14360/298459402.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0manomalies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "anomalies[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_flags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_flags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket and Snowflake parameters\n",
    "snowflake_account = 'at67872.ap-south-1.aws'\n",
    "snowflake_user = 'FYP4'\n",
    "snowflake_password = '!Q2w3e4r5t6y'\n",
    "warehouse='COMPUTE_WH'\n",
    "snowflake_database = 'COAPDATA'\n",
    "snowflake_schema = 'METER_SCHEMA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to Snowflake\n",
    "conn = sf.connect(\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    account=snowflake_account,\n",
    "    warehouse=warehouse,\n",
    "    database=snowflake_database,\n",
    "    schema=snowflake_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "current_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "# Iterate over the anomaly flags and insert them into the Snowflake table\n",
    "for index, flag in enumerate(anomaly_flags):\n",
    "    meter_id = views[index]\n",
    "    anomaly_flag_value = flag\n",
    "    \n",
    "    # Construct the SQL query\n",
    "    query = f\"INSERT INTO anomalies (meterId, anomaly_flag_value, date) VALUES ('{meter_id}', {anomaly_flag_value}, '{current_date}')\"\n",
    "\n",
    "    # Execute the SQL query\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
